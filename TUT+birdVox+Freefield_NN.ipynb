{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hungarian-comment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utworzyd/miniconda3/envs/pytorch_2/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, IterableDataset\n",
    "from torchvision import transforms, datasets\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adaptive-chinese",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mulatwo\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-dream",
   "metadata": {},
   "source": [
    "# Dataset Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stopped-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BirdVoxDataset(Dataset):\n",
    "\n",
    "    # Argument list\n",
    "    # path to the BirdVox-20k csv file\n",
    "    # path to the BirdVox-20k audio files\n",
    "    \n",
    "    def __init__(self, csv_path_B, file_path_B, csv_path_F, file_path_F, csv_path_T, file_path_T):\n",
    "        \n",
    "        csvDataB = pd.read_csv(csv_path_B,dtype = {'hasbird':np.float32})\n",
    "        csvDataF = pd.read_csv(csv_path_F,dtype = {'itemid': 'string','hasbird':np.float32})\n",
    "        csvDataT = pd.read_csv(csv_path_T,dtype = {'itemid': 'string','hasbird':np.float32})\n",
    "        csvData= csvDataB + csvDataF +csvDataT\n",
    "        csvData = pd.concat([csvDataB,csvDataF, csvDataT])\n",
    "        print(csvDataB)\n",
    "        print(csvDataF)\n",
    "        print(csvDataT)\n",
    "        print(csvData)\n",
    "        self.file_names = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for i in range( 0,len(csvData) ):\n",
    "            self.file_names.append(csvData.iloc[i,0])\n",
    "            self.labels.append(csvData.iloc[i,2])\n",
    "            \n",
    "        #tutaj label na float    \n",
    "        self.file_path_B = file_path_B\n",
    "        self.file_path_F = file_path_F\n",
    "        self.file_path_T = file_path_T\n",
    "        \n",
    "        self.lenghtB = csvDataB.size/3\n",
    "        self.lenghtF = csvDataF.size/3\n",
    "        \n",
    "        print(\"Uwaga! Sprawdzenie rozmiaru BirdVoxa: \", self.lenghtB)\n",
    "        self.mel_spectogram = torchaudio.transforms.MelSpectrogram(sample_rate=44100,n_fft=1261, n_mels=80, \n",
    "                                                                   window_fn=torch.hamming_window,\n",
    "                                                                   f_min=50, f_max = 12000)\n",
    "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "        \n",
    "        self.resize = transforms.Resize((80,700))\n",
    "        self.cropp = transforms.CenterCrop((80,700))\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.file_names)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if index < self.lenghtB :\n",
    "            path = self.file_path_B+\"/\"+self.file_names[index]+\".wav\"\n",
    "        else:\n",
    "            if index < self.lenghtB + self.lenghtF:\n",
    "                path = self.file_path_F+\"/\"+self.file_names[index]+\".wav\"\n",
    "            else:\n",
    "                path = self.file_path_T+\"/\"+self.file_names[index]\n",
    "        \n",
    "#         print(path)\n",
    "        #Load audio file into torch.Tensor object. \n",
    "        waveform, sample_rate = torchaudio.load(path)\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        # utworzenie Mal Spektogramu\n",
    "        specgram = self.mel_spectogram(waveform)\n",
    "        \n",
    "        #uwaga! To nie jest za ładne miejsce - możnaby rozwiązać ten problem inaczej chyba\n",
    "        if specgram.size()[2]<700:\n",
    "            specgram  = self.resize(specgram)\n",
    "        else:\n",
    "            specgram  = self.cropp(specgram)\n",
    "            \n",
    "        # transformacja za skali amplitud do decybeli\n",
    "        transformedAmpToDB = self.amplitude_to_db(specgram)\n",
    "\n",
    "        # normalizacja\n",
    "        tensor_minusmean = transformedAmpToDB - transformedAmpToDB.mean()\n",
    "        soundFormatted = tensor_minusmean/tensor_minusmean.abs().max()\n",
    "\n",
    "        return soundFormatted,self.labels[index], self.file_names[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "expected-closer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdVoxDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, csv_path_B, file_path_B,csv_path_F, file_path_F, csv_path_T, file_path_T, batch_size, num_workers):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.csv_path_B = csv_path_B\n",
    "        self.file_path_B = file_path_B\n",
    "        self.csv_path_F = csv_path_F\n",
    "        self.file_path_F = file_path_F\n",
    "        self.csv_path_T = csv_path_T\n",
    "        self.file_path_T = file_path_T\n",
    "        \n",
    "        self.num_workers = num_workers\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        birdvox_dataset = BirdVoxDataset(self.csv_path_B, self.file_path_B,self.csv_path_F, self.file_path_F, self.csv_path_T, self.file_path_T)\n",
    "        self.train_set, self.val_set, self.test_set = torch.utils.data.random_split(birdvox_dataset, [24648,4622,1540], generator=torch.Generator().manual_seed(42))\n",
    "        print(self.val_set[0])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size = self.batch_size, num_workers= self.num_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size = self.batch_size, num_workers= self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size = self.batch_size, num_workers= self.num_workers) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-doctor",
   "metadata": {},
   "source": [
    "# opis sieci\n",
    "##### Input -700x80x1\n",
    "##### Convolution (3x3) -698x78x16\n",
    "##### Pool (3x3) -232x26x16\n",
    "        \n",
    "##### Convolution (3x3) -230x24x16\n",
    "##### Pool (3x3) -76x8x16\n",
    "        \n",
    "##### Convolution (3x3) -74x6x16\n",
    "##### Pool (3x1) -24x6x16\n",
    "        \n",
    "##### Convolution (3x3) -22x4x16\n",
    "##### Pool (3x1)-7x4x16\n",
    "        \n",
    "##### Dense (256) -256\n",
    "##### Dense (32) -32\n",
    "##### Dense (1) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "owned-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Audio_Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolution layers\n",
    "        self.layer1 = torch.nn.Sequential(torch.nn.Conv2d(1,16,kernel_size=3),\n",
    "                                          torch.nn.BatchNorm2d(16),\n",
    "                                          torch.nn.LeakyReLU(0.001),\n",
    "                                          torch.nn.MaxPool2d((3,3)) )\n",
    "        \n",
    "        self.layer2 = torch.nn.Sequential(torch.nn.Conv2d(16,16,kernel_size=3),\n",
    "                                          torch.nn.BatchNorm2d(16),\n",
    "                                          torch.nn.LeakyReLU(0.001),\n",
    "                                          torch.nn.MaxPool2d((3,3)) )\n",
    "        \n",
    "        self.layer3 = torch.nn.Sequential(torch.nn.Conv2d(16,16,kernel_size=3),\n",
    "                                          torch.nn.BatchNorm2d(16),\n",
    "                                          torch.nn.LeakyReLU(0.001),\n",
    "                                          torch.nn.MaxPool2d((1,3)))\n",
    "\n",
    "        self.layer4 = torch.nn.Sequential(torch.nn.Conv2d(16,16,kernel_size=3),\n",
    "                                          torch.nn.BatchNorm2d(16),\n",
    "                                          torch.nn.LeakyReLU(0.001),\n",
    "                                          torch.nn.MaxPool2d((1,3)),\n",
    "                                          torch.nn.Flatten())\n",
    "        \n",
    "        #dense layers\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "        self.fc1 = torch.nn.Linear(7*4*16,256)\n",
    "        self.batch1 = torch.nn.BatchNorm1d(256) \n",
    "        self.leakyReLU = torch.nn.LeakyReLU(0.001)\n",
    "        \n",
    "        self.fc2 = torch.nn.Linear(256,32)\n",
    "        self.batch2 = torch.nn.BatchNorm1d(32) #i na tym leakyRelu\n",
    "        \n",
    "        self.fc3 = torch.nn.Linear(32,1) #i na tym sigmoid\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        self.flatten = torch.nn.Flatten(start_dim=0)\n",
    "        \n",
    "        # compute the accuracy -- no need to roll your own!\n",
    "        self.train_acc = pl.metrics.Accuracy()\n",
    "        self.valid_acc = pl.metrics.Accuracy()\n",
    "        self.test_acc = pl.metrics.Accuracy()\n",
    "        \n",
    "        self.validation_wrong_classified = []\n",
    "        self.validation_wrong_classified_epoch = []\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        #convolution layers\n",
    "        x=self.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "        x=self.layer3(x)\n",
    "        x=self.layer4(x)\n",
    "\n",
    "        #dense layers\n",
    "        x=self.dropout(x)\n",
    "        x=self.fc1(x)\n",
    "        x=self.batch1(x)\n",
    "        x=self.leakyReLU(x)\n",
    "        \n",
    "        x=self.dropout(x)\n",
    "        x=self.fc2(x)\n",
    "        x=self.batch2(x)\n",
    "        x=self.leakyReLU(x)\n",
    "        \n",
    "        x=self.dropout(x)\n",
    "        \n",
    "        x=self.fc3(x)\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        #PYTANIE: czy to tutaj może być, czy jest to problem jednak?\n",
    "        #problematyczny shape tensora dla cross_entropy, dlatego reshape\n",
    "        #był: tensor([[0.4876], ... , [0.4875]]) po: tensor([0.4876, ... ,0.4875])\n",
    "        # reshape na flatten\n",
    "        x=self.flatten(x)\n",
    "        return x\n",
    "    \n",
    "    #z artykułu: The network is trained on binary cross entropy loss using accuracy as a metric.\n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        return F.binary_cross_entropy(logits, labels)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y, f = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_epoch=True, sync_dist=True)\n",
    "        \n",
    "        y = y.int()\n",
    "        accuracy = self.train_acc(logits, y)\n",
    "        self.log('train_acc', self.train_acc, on_epoch=True, sync_dist=True)\n",
    "        \n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y, f = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "#         print('names: ', f)\n",
    "#         print('logits: ', logits)\n",
    "#         print('y: ',y)\n",
    "#         print('loss: ', loss)\n",
    "        y = y.int()\n",
    "        accuracy = self.valid_acc(logits, y)\n",
    "#         print('accuracy: ',accuracy)\n",
    "        \n",
    "        list_file_names = []\n",
    "        #trochę na wprost tworzenie listy tych nagrań, które zostały źle zaklasyfikowane\n",
    "        for id in range(len(f)):\n",
    "            if round(float(logits[id])) != y[id]:\n",
    "                self.validation_wrong_classified_epoch.append(f[id])\n",
    "                \n",
    "        return {'val_loss': loss, 'val_accuracy': accuracy}\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y, f = test_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        y = y.int()\n",
    "        accuracy = self.test_acc(logits, y)\n",
    "        \n",
    "        return {'test_loss': loss, 'test_accuracy': accuracy}\n",
    "    \n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        \n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        avg_accuracy = torch.stack([x['accuracy'] for x in outputs]).mean()\n",
    "\n",
    "        self.log('training_epoch_end_accuracy', avg_accuracy, sync_dist=True)\n",
    "        self.log('training_epoch_end_loss', avg_loss, sync_dist=True)\n",
    "        self.log('lr', self.optimizers().param_groups[0]['lr'], sync_dist=True)\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_accuracy = torch.stack([x['val_accuracy'] for x in outputs]).mean()\n",
    "            \n",
    "        self.log('validation_epoch_end_accuracy', avg_accuracy, sync_dist=True)\n",
    "        self.log('validation_epoch_end_loss', avg_loss, sync_dist=True)\n",
    "        self.validation_wrong_classified.append(self.validation_wrong_classified_epoch.copy())\n",
    "        self.validation_wrong_classified_epoch.clear()\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        avg_accuracy = torch.stack([x['test_accuracy'] for x in outputs]).mean()\n",
    "        \n",
    "        self.log('test_epoch_end_accuracy', avg_accuracy, sync_dist=True)\n",
    "        self.log('test_epoch_end_loss', avg_loss, sync_dist=True)\n",
    "\n",
    "    #według artykułu: For training,ADAM optimizer is used with an initial learning rate of 0.001. \n",
    "    # ! The learning rate was reduced by a factor of 0.2 if there was no improvement in validation accuracy \n",
    "    #over five consecutive epochs.\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.2, patience = 5)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': lr_scheduler,\n",
    "            'monitor': 'validation_epoch_end_loss'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "secondary-auditor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "/home/utworzyd/miniconda3/envs/pytorch_2/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     itemid          datasetid  hasbird\n",
      "0      00053d90-e4b9-4045-a2f1-f39efc90cfa9  BirdVox-DCASE-20k      1.0\n",
      "1      000db435-a40f-4ad9-a74e-d1af284d2c44  BirdVox-DCASE-20k      0.0\n",
      "2      001059c0-e04f-42fc-a8e2-11aad24dc6fb  BirdVox-DCASE-20k      1.0\n",
      "3      00106202-f61e-467d-a80f-070d90421952  BirdVox-DCASE-20k      0.0\n",
      "4      00129593-77ca-40b2-a512-75d178071250  BirdVox-DCASE-20k      0.0\n",
      "...                                     ...                ...      ...\n",
      "19995  fff12db0-9cbe-4155-ac4a-b0b88d84c1d7  BirdVox-DCASE-20k      0.0\n",
      "19996  fff78736-b90a-498e-a18d-a27cd3b83578  BirdVox-DCASE-20k      0.0\n",
      "19997  fff80e7a-7913-4a58-ab9b-4facffe04e56  BirdVox-DCASE-20k      0.0\n",
      "19998  fff847f3-fcfe-43b6-a7c2-85cd84a05cee  BirdVox-DCASE-20k      0.0\n",
      "19999  fffda998-2df9-4055-ab82-2e16b95338a7  BirdVox-DCASE-20k      0.0\n",
      "\n",
      "[20000 rows x 3 columns]\n",
      "      itemid   datasetid  hasbird\n",
      "0      64486  ff1010bird      0.0\n",
      "1       2525  ff1010bird      0.0\n",
      "2      44981  ff1010bird      0.0\n",
      "3     101323  ff1010bird      0.0\n",
      "4     165746  ff1010bird      0.0\n",
      "...      ...         ...      ...\n",
      "7685  168059  ff1010bird      0.0\n",
      "7686  164922  ff1010bird      0.0\n",
      "7687   80789  ff1010bird      1.0\n",
      "7688  104733  ff1010bird      1.0\n",
      "7689   40565  ff1010bird      0.0\n",
      "\n",
      "[7690 rows x 3 columns]\n",
      "                      itemid                                dataid  hasbird\n",
      "0     audio/b109_110_120.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "1      audio/b109_90_100.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "2     audio/b109_100_110.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "3       audio/b109_50_60.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "4       audio/b109_30_40.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "...                      ...                                   ...      ...\n",
      "3115    audio/b081_50_60.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "3116    audio/b081_60_70.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "3117  audio/b081_100_110.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "3118  audio/b081_110_120.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "3119  audio/b081_120_130.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "\n",
      "[3120 rows x 3 columns]\n",
      "                                    itemid          datasetid  hasbird  \\\n",
      "0     00053d90-e4b9-4045-a2f1-f39efc90cfa9  BirdVox-DCASE-20k      1.0   \n",
      "1     000db435-a40f-4ad9-a74e-d1af284d2c44  BirdVox-DCASE-20k      0.0   \n",
      "2     001059c0-e04f-42fc-a8e2-11aad24dc6fb  BirdVox-DCASE-20k      1.0   \n",
      "3     00106202-f61e-467d-a80f-070d90421952  BirdVox-DCASE-20k      0.0   \n",
      "4     00129593-77ca-40b2-a512-75d178071250  BirdVox-DCASE-20k      0.0   \n",
      "...                                    ...                ...      ...   \n",
      "3115                  audio/b081_50_60.wav                NaN      0.0   \n",
      "3116                  audio/b081_60_70.wav                NaN      0.0   \n",
      "3117                audio/b081_100_110.wav                NaN      0.0   \n",
      "3118                audio/b081_110_120.wav                NaN      0.0   \n",
      "3119                audio/b081_120_130.wav                NaN      0.0   \n",
      "\n",
      "                                    dataid  \n",
      "0                                      NaN  \n",
      "1                                      NaN  \n",
      "2                                      NaN  \n",
      "3                                      NaN  \n",
      "4                                      NaN  \n",
      "...                                    ...  \n",
      "3115  TUT-acoustic-scenes-2017-development  \n",
      "3116  TUT-acoustic-scenes-2017-development  \n",
      "3117  TUT-acoustic-scenes-2017-development  \n",
      "3118  TUT-acoustic-scenes-2017-development  \n",
      "3119  TUT-acoustic-scenes-2017-development  \n",
      "\n",
      "[30810 rows x 4 columns]\n",
      "Uwaga! Sprawdzenie rozmiaru BirdVoxa:  20000.0\n",
      "(tensor([[[-0.0650,  0.4110,  0.3952,  ...,  0.4069,  0.4022,  0.3980],\n",
      "         [ 0.1961,  0.1720,  0.1963,  ...,  0.1063,  0.1350,  0.1504],\n",
      "         [ 0.0829, -0.0026, -0.1068,  ...,  0.1019,  0.0355,  0.0011],\n",
      "         ...,\n",
      "         [-0.1411, -0.1164, -0.1987,  ..., -0.1825, -0.1644, -0.1460],\n",
      "         [-0.1631, -0.2485, -0.2525,  ..., -0.2417, -0.3181, -0.2282],\n",
      "         [-0.3321, -0.3835, -0.4531,  ..., -0.4265, -0.4359, -0.3991]]]), 1.0, '7a3a0e03-479e-4e0c-af7a-f63b8939960a')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.20<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">sparkling-thunder-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ulatwo/TUT%2BbirdVox%2BFreefield-NeuralNetwork\" target=\"_blank\">https://wandb.ai/ulatwo/TUT%2BbirdVox%2BFreefield-NeuralNetwork</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ulatwo/TUT%2BbirdVox%2BFreefield-NeuralNetwork/runs/qfm8vpsb\" target=\"_blank\">https://wandb.ai/ulatwo/TUT%2BbirdVox%2BFreefield-NeuralNetwork/runs/qfm8vpsb</a><br/>\n",
       "                Run data is saved locally in <code>/data/home/utworzyd/praca_inz/wandb/run-20210317_232341-qfm8vpsb</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name      | Type        | Params\n",
      "-------------------------------------------\n",
      "0  | layer1    | Sequential  | 192   \n",
      "1  | layer2    | Sequential  | 2.4 K \n",
      "2  | layer3    | Sequential  | 2.4 K \n",
      "3  | layer4    | Sequential  | 2.4 K \n",
      "4  | dropout   | Dropout     | 0     \n",
      "5  | fc1       | Linear      | 114 K \n",
      "6  | batch1    | BatchNorm1d | 512   \n",
      "7  | leakyReLU | LeakyReLU   | 0     \n",
      "8  | fc2       | Linear      | 8.2 K \n",
      "9  | batch2    | BatchNorm1d | 64    \n",
      "10 | fc3       | Linear      | 33    \n",
      "11 | sigmoid   | Sigmoid     | 0     \n",
      "12 | flatten   | Flatten     | 0     \n",
      "13 | train_acc | Accuracy    | 0     \n",
      "14 | valid_acc | Accuracy    | 0     \n",
      "15 | test_acc  | Accuracy    | 0     \n",
      "-------------------------------------------\n",
      "131 K     Trainable params\n",
      "0         Non-trainable params\n",
      "131 K     Total params\n",
      "0.524     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eba6bc4c1b44e1e82a28570b041b2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utworzyd/miniconda3/envs/pytorch_2/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  value = torch.tensor(value, device=device, dtype=torch.float)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb4a050566541dca1a0fa5ab86f8048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdba3947a5db4c1ca920cb08b20a28a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff534696282b461a83fe5e1ec2ca01c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc574d10c04e47af8f04cb1887c13ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a216e278914ba0af4a7673d899c91c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde27fd9b34e4ae8918138293510b9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2352efcb5a5d4f2d82e7458f34e69be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6acf1d48b7d4a8da8111e4702239694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8abe3092741e48759beabf895b29f1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbd4449154a40dfa86422dce66ece13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e928a534bb074d83a6a567b85c6c6e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46310efb311d4ceeae77b1ad0ca7c276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9679c82e37487ab7da3fd30d92729b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd371b51b7b42bca5e850c88f722044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286d5411681a4ceabbd1dc4f4b37876c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08229b927a045fa9a019923fcac2e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b3ede1a4f04c32bbb2a83a4b23e437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f55304aa9b4a14a9e999c45f7c74fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0044a698a02445fa30b29714f2437ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487f4c81740d4707b831a27c607f544f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ae4a0f1eeb4a23922fef3b9a82f5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa92339f0e047059bf12b71cae81e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8007357b72d946668818b302a0f7deb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22380a7f34f54e40b07bc34c4367a976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01832833fee4f158d173007cdd9c31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b751dcf2eb413ea42be08f5d36fa24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     itemid          datasetid  hasbird\n",
      "0      00053d90-e4b9-4045-a2f1-f39efc90cfa9  BirdVox-DCASE-20k      1.0\n",
      "1      000db435-a40f-4ad9-a74e-d1af284d2c44  BirdVox-DCASE-20k      0.0\n",
      "2      001059c0-e04f-42fc-a8e2-11aad24dc6fb  BirdVox-DCASE-20k      1.0\n",
      "3      00106202-f61e-467d-a80f-070d90421952  BirdVox-DCASE-20k      0.0\n",
      "4      00129593-77ca-40b2-a512-75d178071250  BirdVox-DCASE-20k      0.0\n",
      "...                                     ...                ...      ...\n",
      "19995  fff12db0-9cbe-4155-ac4a-b0b88d84c1d7  BirdVox-DCASE-20k      0.0\n",
      "19996  fff78736-b90a-498e-a18d-a27cd3b83578  BirdVox-DCASE-20k      0.0\n",
      "19997  fff80e7a-7913-4a58-ab9b-4facffe04e56  BirdVox-DCASE-20k      0.0\n",
      "19998  fff847f3-fcfe-43b6-a7c2-85cd84a05cee  BirdVox-DCASE-20k      0.0\n",
      "19999  fffda998-2df9-4055-ab82-2e16b95338a7  BirdVox-DCASE-20k      0.0\n",
      "\n",
      "[20000 rows x 3 columns]\n",
      "      itemid   datasetid  hasbird\n",
      "0      64486  ff1010bird      0.0\n",
      "1       2525  ff1010bird      0.0\n",
      "2      44981  ff1010bird      0.0\n",
      "3     101323  ff1010bird      0.0\n",
      "4     165746  ff1010bird      0.0\n",
      "...      ...         ...      ...\n",
      "7685  168059  ff1010bird      0.0\n",
      "7686  164922  ff1010bird      0.0\n",
      "7687   80789  ff1010bird      1.0\n",
      "7688  104733  ff1010bird      1.0\n",
      "7689   40565  ff1010bird      0.0\n",
      "\n",
      "[7690 rows x 3 columns]\n",
      "                      itemid                                dataid  hasbird\n",
      "0     audio/b109_110_120.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "1      audio/b109_90_100.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "2     audio/b109_100_110.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "3       audio/b109_50_60.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "4       audio/b109_30_40.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "...                      ...                                   ...      ...\n",
      "3115    audio/b081_50_60.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "3116    audio/b081_60_70.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "3117  audio/b081_100_110.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "3118  audio/b081_110_120.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "3119  audio/b081_120_130.wav  TUT-acoustic-scenes-2017-development      0.0\n",
      "\n",
      "[3120 rows x 3 columns]\n",
      "                                    itemid          datasetid  hasbird  \\\n",
      "0     00053d90-e4b9-4045-a2f1-f39efc90cfa9  BirdVox-DCASE-20k      1.0   \n",
      "1     000db435-a40f-4ad9-a74e-d1af284d2c44  BirdVox-DCASE-20k      0.0   \n",
      "2     001059c0-e04f-42fc-a8e2-11aad24dc6fb  BirdVox-DCASE-20k      1.0   \n",
      "3     00106202-f61e-467d-a80f-070d90421952  BirdVox-DCASE-20k      0.0   \n",
      "4     00129593-77ca-40b2-a512-75d178071250  BirdVox-DCASE-20k      0.0   \n",
      "...                                    ...                ...      ...   \n",
      "3115                  audio/b081_50_60.wav                NaN      0.0   \n",
      "3116                  audio/b081_60_70.wav                NaN      0.0   \n",
      "3117                audio/b081_100_110.wav                NaN      0.0   \n",
      "3118                audio/b081_110_120.wav                NaN      0.0   \n",
      "3119                audio/b081_120_130.wav                NaN      0.0   \n",
      "\n",
      "                                    dataid  \n",
      "0                                      NaN  \n",
      "1                                      NaN  \n",
      "2                                      NaN  \n",
      "3                                      NaN  \n",
      "4                                      NaN  \n",
      "...                                    ...  \n",
      "3115  TUT-acoustic-scenes-2017-development  \n",
      "3116  TUT-acoustic-scenes-2017-development  \n",
      "3117  TUT-acoustic-scenes-2017-development  \n",
      "3118  TUT-acoustic-scenes-2017-development  \n",
      "3119  TUT-acoustic-scenes-2017-development  \n",
      "\n",
      "[30810 rows x 4 columns]\n",
      "Uwaga! Sprawdzenie rozmiaru BirdVoxa:  20000.0\n",
      "(tensor([[[-0.0650,  0.4110,  0.3952,  ...,  0.4069,  0.4022,  0.3980],\n",
      "         [ 0.1961,  0.1720,  0.1963,  ...,  0.1063,  0.1350,  0.1504],\n",
      "         [ 0.0829, -0.0026, -0.1068,  ...,  0.1019,  0.0355,  0.0011],\n",
      "         ...,\n",
      "         [-0.1411, -0.1164, -0.1987,  ..., -0.1825, -0.1644, -0.1460],\n",
      "         [-0.1631, -0.2485, -0.2525,  ..., -0.2417, -0.3181, -0.2282],\n",
      "         [-0.3321, -0.3835, -0.4531,  ..., -0.4265, -0.4359, -0.3991]]]), 1.0, '7a3a0e03-479e-4e0c-af7a-f63b8939960a')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723f4064bbae4b70a41f26b8fe7bd101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_epoch_end_accuracy': 0.9107142686843872,\n",
      " 'test_epoch_end_loss': 0.2582949697971344}\n",
      "--------------------------------------------------------------------------------\n",
      "[{'test_epoch_end_accuracy': 0.9107142686843872, 'test_epoch_end_loss': 0.2582949697971344}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utworzyd/miniconda3/envs/pytorch_2/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  value = torch.tensor(value, device=device, dtype=torch.float)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 125151<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a302d1c690645ef97215aa9c6c78613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/data/home/utworzyd/praca_inz/wandb/run-20210317_232341-qfm8vpsb/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/data/home/utworzyd/praca_inz/wandb/run-20210317_232341-qfm8vpsb/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train_loss_step</td><td>0.28074</td></tr><tr><td>train_acc_step</td><td>0.90625</td></tr><tr><td>epoch</td><td>24</td></tr><tr><td>_runtime</td><td>5656</td></tr><tr><td>_timestamp</td><td>1616025477</td></tr><tr><td>_step</td><td>38549</td></tr><tr><td>train_loss_epoch</td><td>0.21677</td></tr><tr><td>train_acc_epoch</td><td>0.9219</td></tr><tr><td>training_epoch_end_accuracy</td><td>0.92198</td></tr><tr><td>training_epoch_end_loss</td><td>0.21664</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>validation_epoch_end_accuracy</td><td>0.9057</td></tr><tr><td>validation_epoch_end_loss</td><td>0.25758</td></tr><tr><td>test_epoch_end_accuracy</td><td>0.91071</td></tr><tr><td>test_epoch_end_loss</td><td>0.25829</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train_loss_step</td><td>▅█▄▃▄▅▃▃▆▃▂▆▃▂▃▄▃▂▄▃▃▂▂▂▃▃▁▃▃▃▁▄▁▄▁▂▃▄▂▂</td></tr><tr><td>train_acc_step</td><td>▅▁▆▆▆▆▇▆▃▆▇▅▆▇▇▆▆▇▆▆▇▆▇▇▆▆█▆▆▆▇▆█▅█▇▇▆▇▆</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_acc_epoch</td><td>▁▄▆▆▆▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>training_epoch_end_accuracy</td><td>▁▄▆▆▆▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>training_epoch_end_loss</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_epoch_end_accuracy</td><td>▁▅▆▇▇▇▇█▇█▇██████▇█▇▇▇▇██</td></tr><tr><td>validation_epoch_end_loss</td><td>█▄▃▂▂▂▂▂▂▂▂▁▂▂▁▁▁▂▁▂▂▁▂▂▁</td></tr><tr><td>test_epoch_end_accuracy</td><td>▁</td></tr><tr><td>test_epoch_end_loss</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">sparkling-thunder-1</strong>: <a href=\"https://wandb.ai/ulatwo/TUT%2BbirdVox%2BFreefield-NeuralNetwork/runs/qfm8vpsb\" target=\"_blank\">https://wandb.ai/ulatwo/TUT%2BbirdVox%2BFreefield-NeuralNetwork/runs/qfm8vpsb</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "wandb_logger = WandbLogger(project=\"TUT+birdVox+Freefield-NeuralNetwork\")\n",
    "\n",
    "# przykładowe ścieżki:\n",
    "csv_path_B= './BirdVox/BirdVoxDCASE20k.csv'\n",
    "file_path_B='./BirdVox/data/wav'\n",
    "\n",
    "csv_path_F= './freefield1010/ff1010bird_metadata_2018.csv'\n",
    "file_path_F='./freefield1010/wav'\n",
    "\n",
    "csv_path_T='./TUT-acoustic-scenes-2017/TUT-acoustic-scenes-2017-development/TUT-acoustic-scenes-2017-development.csv'\n",
    "file_path_T='./TUT-acoustic-scenes-2017/TUT-acoustic-scenes-2017-development/'\n",
    "\n",
    "\n",
    "#batch_size ~ 32, 64 [32-128] to standard\n",
    "batch_size = 32\n",
    "\n",
    "#num_workers = 24 if cpu\n",
    "num_workers = 24\n",
    "\n",
    "\n",
    "# z ograniczeniem epok:\n",
    "trainer = pl.Trainer(\n",
    "    logger = wandb_logger,  #W&B integration\n",
    "    log_every_n_steps = 50, #set the logging frequency\n",
    "    max_epochs=25,           #number of epochs  \n",
    "    gpus =0,\n",
    "    progress_bar_refresh_rate=50\n",
    ")\n",
    "\n",
    "birdvox_dm = BirdVoxDataModule(csv_path_B, file_path_B,csv_path_F, file_path_F , csv_path_T, file_path_T, batch_size, num_workers)\n",
    "model = CNN_Audio_Model()\n",
    "\n",
    "trainer.fit(model, birdvox_dm)\n",
    "trainer.save_checkpoint(\"T+B+F_model_25e_vallist.ckpt\")\n",
    "\n",
    "result = trainer.test(model)\n",
    "#UWAGA! Jeszcze nie jest ok, bo epoki!\n",
    "# print('validation_wrong_classified',model.validation_wrong_classified)\n",
    "# print(' ')\n",
    "# print('\\n\\n przedlast_epoch_of_validation: ', model.validation_wrong_classified[-2])\n",
    "# print(' ')\n",
    "# print('\\n\\n last_epoch_of_validation: ', model.validation_wrong_classified[-1])\n",
    "print(result)\n",
    "wandb.finish()\n",
    "\n",
    "# zwykły sposób na zapis tego i odczytanie z pliku\n",
    "with open('T+B+F_wrong_classified_validation_file_names_25e.txt', 'w') as filehandle:\n",
    "    for listitem in model.validation_wrong_classified[-1]:\n",
    "        filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dressed-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "places = []\n",
    "with open('wrong_classified_validation_file_names.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        # remove linebreak which is the last character of the string\n",
    "        currentPlace = line[:-1]\n",
    "\n",
    "        # add item to the list\n",
    "        places.append(currentPlace)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
