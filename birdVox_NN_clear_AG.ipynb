{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "productive-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, IterableDataset\n",
    "from torchvision import transforms, datasets\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "general-valuation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-large",
   "metadata": {},
   "source": [
    "# Dataset Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "quarterly-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BirdVoxDataset(Dataset):\n",
    "\n",
    "    # Argument list\n",
    "    # path to the BirdVox-20k csv file\n",
    "    # path to the BirdVox-20k audio files\n",
    "    \n",
    "    def __init__(self, csv_path,file_path):\n",
    "        \n",
    "        csvData = pd.read_csv(csv_path,dtype = {'hasbird':np.float32})\n",
    "        self.file_names = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for i in range( 0,len(csvData) ):\n",
    "            self.file_names.append(csvData.iloc[i,0])\n",
    "            self.labels.append(csvData.iloc[i,2])\n",
    "            \n",
    "        #tutaj label na float    \n",
    "        self.file_path = file_path\n",
    "        self.mel_spectogram = torchaudio.transforms.MelSpectrogram(sample_rate=44100,n_fft=1261, n_mels=80, \n",
    "                                                                   window_fn=torch.hamming_window,\n",
    "                                                                   f_min=50, f_max = 12000)\n",
    "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.file_names)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        path = self.file_path+\"/\"+self.file_names[index]+\".wav\"\n",
    "        \n",
    "        #Load audio file into torch.Tensor object. \n",
    "        waveform, sample_rate = torchaudio.load(path)\n",
    "        # utworzenie Mal Spektogramu\n",
    "        specgram = self.mel_spectogram(waveform)\n",
    "        # transformacja za skali amplitud do decybeli\n",
    "        transformedAmpToDB = self.amplitude_to_db(specgram)\n",
    "\n",
    "        # normalizacja\n",
    "        tensor_minusmean = transformedAmpToDB - transformedAmpToDB.mean()\n",
    "        soundFormatted = tensor_minusmean/tensor_minusmean.abs().max()\n",
    "\n",
    "        return soundFormatted,self.labels[index], self.file_names[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "empty-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdVoxDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, csv_path, file_path, batch_size, num_workers):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.csv_path = csv_path\n",
    "        self.file_path = file_path\n",
    "        self.num_workers = num_workers\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        birdvox_dataset = BirdVoxDataset(self.csv_path, self.file_path)\n",
    "        self.train_set, self.val_set, self.test_set = torch.utils.data.random_split(birdvox_dataset, [16000,3000,1000], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size = self.batch_size, num_workers= self.num_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size = self.batch_size, num_workers= self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size = self.batch_size, num_workers= self.num_workers) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "enclosed-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nie do końca wiem, jak działa inicjalizacja wag\n",
    "def weights_init_kaiming(m):\n",
    "    classname = m.__class__.__name__\n",
    "\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
    "    elif classname.find('Linear') != -1:\n",
    "        torch.nn.init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant(m.bias.data, 0.0)\n",
    "\n",
    "#typ xavier i orthogonal nie jest używany\n",
    "# sprawdzić -jak działa typ kaiming (bo to on jest używany)\n",
    "def init_weights(net, init_type='normal'):\n",
    "    if init_type == 'normal':\n",
    "        net.apply(weights_init_normal)\n",
    "#     elif init_type == 'xavier':\n",
    "#         net.apply(weights_init_xavier)\n",
    "    elif init_type == 'kaiming':\n",
    "        net.apply(weights_init_kaiming)\n",
    "#     elif init_type == 'orthogonal':\n",
    "#         net.apply(weights_init_orthogonal)\n",
    "    else:\n",
    "        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "stretch-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dla przypomnienia jakie argumenty są podawane w CNN_Audio_Model\n",
    "# argumenty uzupełnione na podstawie podanego przykładu zastosowania, w:\n",
    "#    https://github.com/ozan-oktay/Attention-Gated-Networks/blob/master/models/networks/sononet_grid_attention.py\n",
    "#  self.compatibility_score1 = AttentionBlock2D(in_channels = filters[2], \n",
    "#                                                      gating_channels = filters[3],\n",
    "#                                                      inter_channels = filters[3],\n",
    "#                                                      sub_sample_factor = (1,1),\n",
    "#                                                      mode = 'concatenation',\n",
    "#                                                      use_W=False,\n",
    "#                                                      use_phi=True, use_theta=True\n",
    "#                                                      use_psi = True,\n",
    "#                                                      nonlinearity1 = 'relu'\n",
    "#                                                     )\n",
    "\n",
    "class _GridAttentionBlockND_TORR(torch.nn.Module):\n",
    "    def __init__(self, in_channels, gating_channels, inter_channels=None, dimension=2, mode='concatenation',\n",
    "                 sub_sample_factor=(1,1,1), bn_layer=True, use_W=True, use_phi=True, use_theta=True, use_psi=True, nonlinearity1='relu'):\n",
    "        \n",
    "        super(_GridAttentionBlockND_TORR, self).__init__()\n",
    "\n",
    "        assert dimension==2\n",
    "        assert mode in ['concatenation', 'concatenation_softmax',\n",
    "                        'concatenation_sigmoid', 'concatenation_mean',\n",
    "                        'concatenation_range_normalise', 'concatenation_mean_flow']\n",
    "\n",
    "        # Default parameter set\n",
    "        self.mode = mode\n",
    "        \n",
    "        #w sumie to u mnie akurat dimension jest zawsze 2... można to potem zrefaktoryzować ;) \n",
    "        self.dimension = dimension\n",
    "        \n",
    "        #chyba nie do końca wiem, czym jest sub_sample_factor\n",
    "        self.sub_sample_factor = sub_sample_factor if isinstance(sub_sample_factor, tuple) else tuple([sub_sample_factor])*dimension\n",
    "        self.sub_sample_kernel_size = self.sub_sample_factor\n",
    "\n",
    "        # Number of channels (pixel dimensions)\n",
    "        self.in_channels = in_channels\n",
    "        self.gating_channels = gating_channels\n",
    "        self.inter_channels = inter_channels\n",
    "\n",
    "        ## to w naszym przypadku nie jest potrzebne - in_channels są podane\n",
    "        ##     aczkolwiek dobrze byłoby się dowiedzieć, dlaczego w podanym przypadku to inter_channels = gating_channels (a nie wejście na pół...)\n",
    "        if self.inter_channels is None:\n",
    "            self.inter_channels = in_channels // 2\n",
    "            if self.inter_channels == 0:\n",
    "                self.inter_channels = 1\n",
    "\n",
    "                \n",
    "        conv_nd = nn.Conv2d\n",
    "        bn = nn.BatchNorm2d\n",
    "        self.upsample_mode = 'bilinear'\n",
    "\n",
    "\n",
    "        # initialise id functions\n",
    "        # Theta^T * x_ij + Phi^T * gating_signal + bias\n",
    "        self.W = lambda x: x\n",
    "        self.theta = lambda x: x\n",
    "        self.psi = lambda x: x\n",
    "        self.phi = lambda x: x\n",
    "        self.nl1 = lambda x: x\n",
    "\n",
    "        # use_W jest podane jako False\n",
    "        \n",
    "#         if use_W:\n",
    "#             if bn_layer:\n",
    "#                 self.W = nn.Sequential(\n",
    "#                     conv_nd(in_channels=self.in_channels, out_channels=self.in_channels, kernel_size=1, stride=1, padding=0),\n",
    "#                     bn(self.in_channels),\n",
    "#                 )\n",
    "#             else:\n",
    "#                 self.W = conv_nd(in_channels=self.in_channels, out_channels=self.in_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        #czy to use_theta jest równoznaczne z używaniem W^T ?\n",
    "        if use_theta:\n",
    "            self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                                 kernel_size=self.sub_sample_kernel_size, stride=self.sub_sample_factor, padding=0, bias=False)\n",
    "\n",
    "        # # # # # # # # # # #\n",
    "        if use_phi:\n",
    "            self.phi = conv_nd(in_channels=self.gating_channels, out_channels=self.inter_channels,\n",
    "                               kernel_size=self.sub_sample_kernel_size, stride=self.sub_sample_factor, padding=0, bias=False)\n",
    "\n",
    "        if use_psi:\n",
    "            self.psi = conv_nd(in_channels=self.inter_channels, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "\n",
    "        if nonlinearity1:\n",
    "            if nonlinearity1 == 'relu':\n",
    "                self.nl1 = lambda x: F.relu(x, inplace=True)\n",
    "\n",
    "        if 'concatenation' in mode:\n",
    "            self.operation_function = self._concatenation\n",
    "        else:\n",
    "            raise NotImplementedError('Unknown operation function.')\n",
    "\n",
    "        # Initialise weights\n",
    "        # Co to jest self.children ?\n",
    "        # z dokumentacji: Returns an iterator over immediate children modules.\n",
    "        for m in self.children():\n",
    "            init_weights(m, init_type='kaiming')\n",
    "\n",
    "            \n",
    "        # to też nie jest używane w podanym przypadku\n",
    "        \n",
    "#         if use_psi and self.mode == 'concatenation_sigmoid':\n",
    "#             nn.init.constant(self.psi.bias.data, 3.0)\n",
    "\n",
    "#         if use_psi and self.mode == 'concatenation_softmax':\n",
    "#             nn.init.constant(self.psi.bias.data, 10.0)\n",
    "\n",
    "\n",
    "        # if use_psi and self.mode == 'concatenation_mean':\n",
    "        #     nn.init.constant(self.psi.bias.data, 3.0)\n",
    "\n",
    "        # if use_psi and self.mode == 'concatenation_range_normalise':\n",
    "        #     nn.init.constant(self.psi.bias.data, 3.0)\n",
    "\n",
    "        \n",
    "        # trochę bez sensu fragment kodu - czy tutaj powinno być jakoś inaczej?\n",
    "#         parallel = False\n",
    "#         if parallel:\n",
    "#             if use_W: self.W = nn.DataParallel(self.W)\n",
    "#             if use_phi: self.phi = nn.DataParallel(self.phi)\n",
    "#             if use_psi: self.psi = nn.DataParallel(self.psi)\n",
    "#             if use_theta: self.theta = nn.DataParallel(self.theta)\n",
    "\n",
    "    def forward(self, x, g):\n",
    "        '''\n",
    "        :param x: (b, c, t, h, w)\n",
    "        :param g: (b, g_d)\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        output = self.operation_function(x, g)  ## w naszym przypadku _concatenation(x ,g )\n",
    "        return output\n",
    "\n",
    "    def _concatenation(self, x, g):\n",
    "        input_size = x.size()\n",
    "        batch_size = input_size[0]\n",
    "        assert batch_size == g.size(0)\n",
    "\n",
    "        #############################\n",
    "        # compute compatibility score\n",
    "\n",
    "        # theta => (b, c, t, h, w) -> (b, i_c, t, h, w)\n",
    "        # phi   => (b, c, t, h, w) -> (b, i_c, t, h, w)\n",
    "        theta_x = self.theta(x)\n",
    "        theta_x_size = theta_x.size()\n",
    "\n",
    "        #  nl(theta.x + phi.g + bias) -> f = (b, i_c, t/s1, h/s2, w/s3)\n",
    "\n",
    "        # to chyba przepróbkowanie g, żeby było wielkości theta\n",
    "        # pytanie: dlaczego na g jest jeszcze raz przez conv2d ?\n",
    "        phi_g = F.upsample(self.phi(g), size=theta_x_size[2:], mode=self.upsample_mode)\n",
    "\n",
    "        f = theta_x + phi_g\n",
    "        f = self.nl1(f)  #sigmoid1 - relu\n",
    "\n",
    "        psi_f = self.psi(f)  #conv2d\n",
    "        \n",
    "        #koniec pierwszej części równania\n",
    "\n",
    "        ############################################\n",
    "        # Zaraz, zaraz...\n",
    "        # Tam było podane jako mode concatenation - a to zdaje się powoduje w tym miejscu błąd...\n",
    "        # To jest po prostu normalizacja (w równaniu oznaczona sigma2) \n",
    "        # zastosuję wersję dla concatenation_range_normalise\n",
    "        \n",
    "#         # normalisation -- scale compatibility score\n",
    "#         #  psi^T . f -> (b, 1, t/s1, h/s2, w/s3)\n",
    "#         if self.mode == 'concatenation_softmax':\n",
    "#             sigm_psi_f = F.softmax(psi_f.view(batch_size, 1, -1), dim=2)\n",
    "#             sigm_psi_f = sigm_psi_f.view(batch_size, 1, *theta_x_size[2:])\n",
    "#         elif self.mode == 'concatenation_mean':\n",
    "#             psi_f_flat = psi_f.view(batch_size, 1, -1)\n",
    "#             psi_f_sum = torch.sum(psi_f_flat, dim=2)#clamp(1e-6)\n",
    "#             psi_f_sum = psi_f_sum[:,:,None].expand_as(psi_f_flat)\n",
    "\n",
    "#             sigm_psi_f = psi_f_flat / psi_f_sum\n",
    "#             sigm_psi_f = sigm_psi_f.view(batch_size, 1, *theta_x_size[2:])\n",
    "#         elif self.mode == 'concatenation_mean_flow':\n",
    "#             psi_f_flat = psi_f.view(batch_size, 1, -1)\n",
    "#             ss = psi_f_flat.shape\n",
    "#             psi_f_min = psi_f_flat.min(dim=2)[0].view(ss[0],ss[1],1)\n",
    "#             psi_f_flat = psi_f_flat - psi_f_min\n",
    "#             psi_f_sum = torch.sum(psi_f_flat, dim=2).view(ss[0],ss[1],1).expand_as(psi_f_flat)\n",
    "\n",
    "#             sigm_psi_f = psi_f_flat / psi_f_sum\n",
    "#             sigm_psi_f = sigm_psi_f.view(batch_size, 1, *theta_x_size[2:])\n",
    "#         elif self.mode == 'concatenation_range_normalise':\n",
    "#             psi_f_flat = psi_f.view(batch_size, 1, -1)\n",
    "#             ss = psi_f_flat.shape\n",
    "#             psi_f_max = torch.max(psi_f_flat, dim=2)[0].view(ss[0], ss[1], 1)\n",
    "#             psi_f_min = torch.min(psi_f_flat, dim=2)[0].view(ss[0], ss[1], 1)\n",
    "\n",
    "#             sigm_psi_f = (psi_f_flat - psi_f_min) / (psi_f_max - psi_f_min).expand_as(psi_f_flat)\n",
    "#             sigm_psi_f = sigm_psi_f.view(batch_size, 1, *theta_x_size[2:])\n",
    "\n",
    "#         elif self.mode == 'concatenation_sigmoid':\n",
    "#             sigm_psi_f = F.sigmoid(psi_f)\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "\n",
    "        # na coś muszę ustawić sigm_psi_f, więc dam to co w 'concatenation_range_normalise'\n",
    "        # to jest zdaje się drugie równanie, czyli na tym, co wcześniej zostało policzone dokonywana jest NORMALIZACJA\n",
    "        psi_f_flat = psi_f.view(batch_size, 1, -1)\n",
    "        ss = psi_f_flat.shape\n",
    "        psi_f_max = torch.max(psi_f_flat, dim=2)[0].view(ss[0], ss[1], 1)\n",
    "        psi_f_min = torch.min(psi_f_flat, dim=2)[0].view(ss[0], ss[1], 1)\n",
    "\n",
    "        sigm_psi_f = (psi_f_flat - psi_f_min) / (psi_f_max - psi_f_min).expand_as(psi_f_flat)\n",
    "        sigm_psi_f = sigm_psi_f.view(batch_size, 1, *theta_x_size[2:])\n",
    "        \n",
    "\n",
    "        # sigm_psi_f is attention map! upsample the attentions and multiply\n",
    "        sigm_psi_f = F.upsample(sigm_psi_f, size=input_size[2:], mode=self.upsample_mode)\n",
    "        y = sigm_psi_f.expand_as(x) * x\n",
    "        W_y = self.W(y)\n",
    "\n",
    "        return W_y, sigm_psi_f\n",
    "\n",
    "#AttentionBlock2D\n",
    "class GridAttentionBlock2D_TORR(_GridAttentionBlockND_TORR):\n",
    "        def __init__(self, in_channels, gating_channels, inter_channels=None, mode='concatenation',\n",
    "                 sub_sample_factor=(1,1), bn_layer=True,\n",
    "                 use_W=True, use_phi=True, use_theta=True, use_psi=True,\n",
    "                 nonlinearity1='relu'):\n",
    "            super(GridAttentionBlock2D_TORR, self).__init__(in_channels,\n",
    "                                                   inter_channels=inter_channels,\n",
    "                                                   gating_channels=gating_channels,\n",
    "                                                   dimension=2, mode=mode,\n",
    "                                                   sub_sample_factor=sub_sample_factor,\n",
    "                                                   bn_layer=bn_layer,\n",
    "                                                   use_W=use_W,\n",
    "                                                   use_phi=use_phi,\n",
    "                                                   use_theta=use_theta,\n",
    "                                                   use_psi=use_psi,\n",
    "                                                   nonlinearity1=nonlinearity1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-entertainment",
   "metadata": {},
   "source": [
    "# opis sieci\n",
    "##### Input -700x80x1\n",
    "##### Convolution (3x3) -698x78x16\n",
    "##### Pool (3x3) -232x26x16\n",
    "        \n",
    "##### Convolution (3x3) -230x24x16\n",
    "##### Pool (3x3) -76x8x16\n",
    "        \n",
    "##### Convolution (3x3) -74x6x16\n",
    "##### Pool (3x1) -24x6x16\n",
    "        \n",
    "##### Convolution (3x3) -22x4x16\n",
    "##### Pool (3x1)-7x4x16\n",
    "        \n",
    "##### Dense (256) -256\n",
    "##### Dense (32) -32\n",
    "##### Dense (1) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "northern-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Audio_Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        ##templatka:\n",
    "        ## Conv2d(int(in_channels), int(n_filters), kernel_size)\n",
    "        \n",
    "        #nie wiem w sumie czy robienie tych filters ma jakiś głębszy sens w tym przypadku, skoro wszędzie jest 16\n",
    "        filters = [16,16,16,16]\n",
    "        \n",
    "        self.prediction_changed_by_AG = 0\n",
    "        self.prediction_changed_by_AG_training = []       \n",
    "        self.prediction_changed_by_AG_validation = []     \n",
    "        self.prediction_changed_by_AG_testing = []\n",
    "        \n",
    "        # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "        #convolution layers\n",
    "        self.layer1 = torch.nn.Sequential(torch.nn.Conv2d(1,16,kernel_size=3),\n",
    "                                          torch.nn.BatchNorm2d(16),\n",
    "                                          torch.nn.LeakyReLU(0.001),\n",
    "                                          torch.nn.MaxPool2d((3,3)) )\n",
    "        \n",
    "        self.layer2 = torch.nn.Sequential(torch.nn.Conv2d(16,16,kernel_size=3),\n",
    "                                          torch.nn.BatchNorm2d(16),\n",
    "                                          torch.nn.LeakyReLU(0.001),\n",
    "                                          torch.nn.MaxPool2d((3,3)) )\n",
    "        \n",
    "        self.layer3 = torch.nn.Sequential(torch.nn.Conv2d(16,16,kernel_size=3),\n",
    "                                          torch.nn.BatchNorm2d(16),\n",
    "                                          torch.nn.LeakyReLU(0.001),\n",
    "                                          torch.nn.MaxPool2d((1,3)))\n",
    "\n",
    "        self.layer4 = torch.nn.Sequential(torch.nn.Conv2d(16,16,kernel_size=3),\n",
    "                                          torch.nn.BatchNorm2d(16),\n",
    "                                          torch.nn.LeakyReLU(0.001),\n",
    "                                          torch.nn.MaxPool2d((1,3))\n",
    "                                         # torch.nn.Flatten()\n",
    "                                         )\n",
    "        \n",
    "        ## czy tu wystarczy tak to rozdzielić, czy trzeba g pozyskiwać jeszcze nieco wcześniej?\n",
    "        # czy to końcowe Flatten powinno już być w dense_layers\n",
    "        self.flatten_afterConv4 = torch.nn.Flatten()\n",
    "        \n",
    "        #dense layers\n",
    "        self.dense_input_size = 7*4*16\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "        self.fc1 = torch.nn.Linear(self.dense_input_size,256)\n",
    "        self.batch1 = torch.nn.BatchNorm1d(256) \n",
    "        self.leakyReLU = torch.nn.LeakyReLU(0.001)\n",
    "        \n",
    "        self.fc2 = torch.nn.Linear(256,32)\n",
    "        self.batch2 = torch.nn.BatchNorm1d(32) #i na tym leakyRelu\n",
    "        \n",
    "        self.fc3 = torch.nn.Linear(32,1) #i na tym sigmoid\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        self.flatten = torch.nn.Flatten(start_dim=0)\n",
    "        \n",
    "        # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "        #attention maps\n",
    "        \n",
    "        #trochę nie do końca wiem czym jest szereg tych argumentów\n",
    "        self.compatibility_score1 = GridAttentionBlock2D_TORR(in_channels = filters[2], \n",
    "                                                     gating_channels = filters[3],\n",
    "                                                     inter_channels = filters[3],\n",
    "                                                     sub_sample_factor = (1,1),\n",
    "                                                     mode = 'concatenation',\n",
    "                                                     use_W=False,\n",
    "                                                     use_phi=True, use_theta = True,\n",
    "                                                     use_psi = True,\n",
    "                                                     nonlinearity1 = 'relu'\n",
    "                                                    )\n",
    "        self.compatibility_score2 = GridAttentionBlock2D_TORR(in_channels = filters[3], \n",
    "                                                     gating_channels = filters[3],\n",
    "                                                     inter_channels = filters[3],\n",
    "                                                     sub_sample_factor = (1,1),\n",
    "                                                     mode = 'concatenation',\n",
    "                                                     use_W=False,\n",
    "                                                     use_phi=True,use_theta = True,\n",
    "                                                     use_psi = True,\n",
    "                                                     nonlinearity1 = 'relu'\n",
    "                                                    )\n",
    "        \n",
    "        # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "        # Aggragation Strategies \n",
    "        \n",
    "        self.attention_filter_sizes = [filters[2],filters[3]]\n",
    "        n_classes = 2\n",
    "        \n",
    "        self.classifier = nn.Linear(filters[2]+filters[3]+filters[3], n_classes)\n",
    "        self.classifier1 = nn.Linear(filters[2], 1)\n",
    "        self.classifier2 = nn.Linear(filters[3], 1)\n",
    "        self.classifier3 = nn.Linear(filters[3], 1)\n",
    "        self.classifiers = [self.classifier1, self.classifier2, self.classifier3]\n",
    "\n",
    "        self.aggregate = self.aggregation_concat_with_dense\n",
    "        \n",
    "        # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "        # initialise weights\n",
    "        #to, co jest tam, a czego do końca nie rozumiem... (i co to self.modules? )\n",
    "        # The self.modules() method returns an iterable to the many layers or “modules” defined in the model class.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                init_weights(m, init_type = 'kaiming')\n",
    "            elif isinstance(m, torch.nn.BatchNorm2d):\n",
    "                init_weights(m, init_type = 'kaiming')\n",
    "        \n",
    "        \n",
    "        # compute the accuracy -- no need to roll your own!\n",
    "        self.train_acc = pl.metrics.Accuracy()\n",
    "        self.valid_acc = pl.metrics.Accuracy()\n",
    "        self.test_acc = pl.metrics.Accuracy()\n",
    "        \n",
    "        self.validation_wrong_classified = []\n",
    "        self.validation_wrong_classified_epoch = []\n",
    "        \n",
    "    # ***************************************************** #\n",
    "        \n",
    "    # attended_maps - (g1,g2,g)\n",
    "    \n",
    "    #to jest agragacja, gdy po AG na wyjściu robimy dense_layers\n",
    "    # liczona jest średnia z trzech wyjść\n",
    "    def aggregation_concat_with_dense(self, *attended_maps):\n",
    "\n",
    "    #jeśli jest dense\n",
    "        a1 = attended_maps[0].reshape( (1,int( attended_maps[0].size()[0] ) ) )\n",
    "        a2 = attended_maps[1].reshape((1,int( attended_maps[1].size()[0]) )) \n",
    "        a3 = attended_maps[2].reshape((1,int( attended_maps[2].size()[0]) ))\n",
    "        aggregation_cat = torch.cat((a1,a2,a3), dim=0)\n",
    "\n",
    "        aggregation_mean = torch.mean(aggregation_cat, 0)\n",
    "\n",
    "\n",
    "        for idx,mean_v in enumerate(aggregation_mean):\n",
    "            if round(float(mean_v)) != round(float(attended_maps[2][idx])):\n",
    "                self.prediction_changed_by_AG += 1\n",
    "\n",
    "        return aggregation_mean\n",
    "    \n",
    "    \n",
    "    # ***************************************************** #\n",
    "    # próby przeprowadzenia agregacji, gdy warstwy po AG nie przechodzą przez dense_layers\n",
    "    # lecz są to raczej próby - jak na razie nie działa\n",
    "    def aggregation_concat_standard(self, *attended_maps):\n",
    "        print(\"a1: \", attended_maps[0])\n",
    "        print(\"a2: \", attended_maps[1])\n",
    "        print(\"a3: \", attended_maps[2])\n",
    "        c1 = self.classifier1(attended_maps[0])\n",
    "        c2 = self.classifier2(attended_maps[1])\n",
    "#         c3 = self.classifier3(attended_maps[2])\n",
    "        \n",
    "        print(\"c1: \", c1)\n",
    "        print(\"c2: \", c2)\n",
    "#         print(\"c3: \", attended_maps[2])\n",
    "\n",
    "#         #jeśli nie ma\n",
    "        c1 = c1.reshape((1,int( c1.size()[0]) ))\n",
    "        c2 = c2.reshape((1,int( c2.size()[0]) ))\n",
    "        g = attended_maps[2].reshape((1,int( attended_maps[2].size()[0]) ))\n",
    "        print(\"c1 after: \", c1)\n",
    "        aggregation_cat = torch.cat((c1,c2,g), dim=0)\n",
    "        print(\"aggregation_cat: \", aggregation_cat)\n",
    "\n",
    "        aggregation_mean = torch.mean(aggregation_cat, 0)\n",
    "        print(\"aggregation_mean: \", aggregation_mean)\n",
    "\n",
    "        \n",
    "        return aggregation_mean\n",
    "    \n",
    "#         result = [ clf(att) for clf, att in zip(self.classifiers, attended_maps) ]\n",
    "#         return result\n",
    "#         return self.classifier(torch.cat(attended_maps, dim=1))\n",
    "    \n",
    "    # ***************************************************** #\n",
    "    def dense_layers(self,inputs, input_size):\n",
    "        self.dense_input_size = input_size\n",
    "        #dense layers\n",
    "        \n",
    "        # czy tutaj to flatten ok?\n",
    "        x=self.flatten_afterConv4(inputs)\n",
    "        x=self.dropout(x)\n",
    "        \n",
    "        #to w sumie można jakoś rozpisać, żeby tego Linear tutaj nie inicjalizować\n",
    "        self.fc1 = torch.nn.Linear(self.dense_input_size,256)\n",
    "        x=self.fc1(x)\n",
    "        x=self.batch1(x)\n",
    "        x=self.leakyReLU(x)\n",
    "        \n",
    "        x=self.dropout(x)\n",
    "        x=self.fc2(x)\n",
    "        x=self.batch2(x)\n",
    "        x=self.leakyReLU(x)\n",
    "        \n",
    "        x=self.dropout(x)\n",
    "        \n",
    "        x=self.fc3(x)\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        x=self.flatten(x)\n",
    "        return x\n",
    "        \n",
    "    # ***************************************************** #\n",
    "    def forward(self,inputs):\n",
    "        \n",
    "        #convolution layers\n",
    "        conv_layer1=self.layer1(inputs)\n",
    "        conv_layer2=self.layer2(conv_layer1)\n",
    "        conv_layer3=self.layer3(conv_layer2)\n",
    "        conv_layer4=self.layer4(conv_layer3)\n",
    "        \n",
    "\n",
    "        after_dense = self.dense_layers(conv_layer4, 7*4*16)\n",
    "        \n",
    "        ## Attention Mechanism\n",
    "        \n",
    "        #czy tutaj właściwie nie powinno być rozdzielenie conv_layer4 tak, żeby była przekazywana jako g bez maxpoolingu? albo bez flatten\n",
    "        g_conv1, att1 = self.compatibility_score1(conv_layer2 ,conv_layer4)\n",
    "        g_conv2, att2 = self.compatibility_score2(conv_layer3,conv_layer4)\n",
    "        \n",
    "        ## dense on attention filters\n",
    "        # Pytanie: czy na g_conv1 i g_conv2 też dense_layers? Czy tak jak w tamtym kodzie:\n",
    "#         fsizes = self.attention_filter_sizes\n",
    "#         g1 = torch.sum(g_conv1.view(batch_size, fsizes[0], -1), dim=-1)\n",
    "#         g2 = torch.sum(g_conv2.view(batch_size, fsizes[0], -1), dim=-1)\n",
    "        \n",
    "        # wersja z dense_layers na warstwach po self.compatibility_score1 (mechaniźmie AG)\n",
    "\n",
    "        g1 = self.dense_layers(g_conv1, 76*8*16)\n",
    "        g2 = self.dense_layers(g_conv2, 24*6*16)\n",
    "        \n",
    "        output = self.aggregate(g1,g2,after_dense)\n",
    "        return output\n",
    "    \n",
    "\n",
    "    # ***************************************************** #\n",
    "    #z artykułu: The network is trained on binary cross entropy loss using accuracy as a metric.\n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        return F.binary_cross_entropy(logits, labels)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y, f = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_epoch=True, sync_dist=True)\n",
    "        \n",
    "        y = y.int()\n",
    "        accuracy = self.train_acc(logits, y)\n",
    "        self.log('train_acc', self.train_acc, on_epoch=True, sync_dist=True)\n",
    "        \n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y, f = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "\n",
    "        y = y.int()\n",
    "        accuracy = self.valid_acc(logits, y)\n",
    "        \n",
    "        list_file_names = []\n",
    "        #trochę na wprost tworzenie listy tych nagrań, które zostały źle zaklasyfikowane\n",
    "        for id in range(len(f)):\n",
    "            if round(float(logits[id])) != y[id]:\n",
    "                self.validation_wrong_classified_epoch.append(f[id])\n",
    "                \n",
    "        return {'val_loss': loss, 'val_accuracy': accuracy}\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y, f = test_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        y = y.int()\n",
    "        accuracy = self.test_acc(logits, y)\n",
    "        \n",
    "        return {'test_loss': loss, 'test_accuracy': accuracy}\n",
    "    \n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        \n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        avg_accuracy = torch.stack([x['accuracy'] for x in outputs]).mean()\n",
    "\n",
    "        self.log('training_epoch_end_accuracy', avg_accuracy, sync_dist=True)\n",
    "        self.log('training_epoch_end_loss', avg_loss, sync_dist=True)\n",
    "        self.log('lr', self.optimizers().param_groups[0]['lr'], sync_dist=True)\n",
    "        \n",
    "        self.prediction_changed_by_AG_training.append(self.prediction_changed_by_AG)\n",
    "        self.prediction_changed_by_AG = 0\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_accuracy = torch.stack([x['val_accuracy'] for x in outputs]).mean()\n",
    "            \n",
    "        self.log('validation_epoch_end_accuracy', avg_accuracy, sync_dist=True)\n",
    "        self.log('validation_epoch_end_loss', avg_loss, sync_dist=True)\n",
    "        self.validation_wrong_classified.append(self.validation_wrong_classified_epoch.copy())\n",
    "        self.validation_wrong_classified_epoch.clear()\n",
    "        \n",
    "        self.prediction_changed_by_AG_validation.append(self.prediction_changed_by_AG)\n",
    "        self.prediction_changed_by_AG = 0\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        avg_accuracy = torch.stack([x['test_accuracy'] for x in outputs]).mean()\n",
    "        \n",
    "        self.log('test_epoch_end_accuracy', avg_accuracy, sync_dist=True)\n",
    "        self.log('test_epoch_end_loss', avg_loss, sync_dist=True)\n",
    "        \n",
    "        self.prediction_changed_by_AG_testing.append(self.prediction_changed_by_AG)\n",
    "        self.prediction_changed_by_AG = 0\n",
    "\n",
    "    #według artykułu: For training,ADAM optimizer is used with an initial learning rate of 0.001. \n",
    "    # ! The learning rate was reduced by a factor of 0.2 if there was no improvement in validation accuracy \n",
    "    #over five consecutive epochs.\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.2, patience = 5)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': lr_scheduler,\n",
    "            'monitor': 'validation_epoch_end_loss'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "comprehensive-residence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "<ipython-input-59-db7d1f0f0eb5>:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  torch.nn.init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
      "<ipython-input-59-db7d1f0f0eb5>:10: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(m.weight.data, 1.0, 0.02)\n",
      "<ipython-input-59-db7d1f0f0eb5>:11: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  torch.nn.init.constant(m.bias.data, 0.0)\n",
      "\n",
      "   | Name                 | Type                      | Params\n",
      "--------------------------------------------------------------------\n",
      "0  | layer1               | Sequential                | 192   \n",
      "1  | layer2               | Sequential                | 2.4 K \n",
      "2  | layer3               | Sequential                | 2.4 K \n",
      "3  | layer4               | Sequential                | 2.4 K \n",
      "4  | flatten_afterConv4   | Flatten                   | 0     \n",
      "5  | dropout              | Dropout                   | 0     \n",
      "6  | fc1                  | Linear                    | 114 K \n",
      "7  | batch1               | BatchNorm1d               | 512   \n",
      "8  | leakyReLU            | LeakyReLU                 | 0     \n",
      "9  | fc2                  | Linear                    | 8.2 K \n",
      "10 | batch2               | BatchNorm1d               | 64    \n",
      "11 | fc3                  | Linear                    | 33    \n",
      "12 | sigmoid              | Sigmoid                   | 0     \n",
      "13 | flatten              | Flatten                   | 0     \n",
      "14 | compatibility_score1 | GridAttentionBlock2D_TORR | 529   \n",
      "15 | compatibility_score2 | GridAttentionBlock2D_TORR | 529   \n",
      "16 | classifier           | Linear                    | 98    \n",
      "17 | classifier1          | Linear                    | 17    \n",
      "18 | classifier2          | Linear                    | 17    \n",
      "19 | classifier3          | Linear                    | 17    \n",
      "20 | train_acc            | Accuracy                  | 0     \n",
      "21 | valid_acc            | Accuracy                  | 0     \n",
      "22 | test_acc             | Accuracy                  | 0     \n",
      "--------------------------------------------------------------------\n",
      "132 K     Trainable params\n",
      "0         Non-trainable params\n",
      "132 K     Total params\n",
      "0.529     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010be6d413d149b393049e9a45bbbf63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utworzyd/miniconda3/envs/pytorch_2/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  value = torch.tensor(value, device=device, dtype=torch.float)\n",
      "/home/utworzyd/miniconda3/envs/pytorch_2/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da7ca38a9964856939b3d36e68cae49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd8805d255c41b98b4f0bf7c51ea1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utworzyd/miniconda3/envs/pytorch_2/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13db065d1f34de3a442e083960581ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_epoch_end_accuracy': 0.58984375,\n",
      " 'test_epoch_end_loss': 0.6790212392807007}\n",
      "--------------------------------------------------------------------------------\n",
      "[{'test_epoch_end_accuracy': 0.58984375, 'test_epoch_end_loss': 0.6790212392807007}]\n",
      "prediction_changed_by_AG_training:  [4409]\n",
      "prediction_changed_by_AG_validation:  [0, 403]\n",
      "prediction_changed_by_AG_testing :  [126]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 69674<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3970183a4f3d4c228542d01e5649b5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/data/home/utworzyd/praca_inz/wandb/run-20210415_115046-1rtpcijt/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/data/home/utworzyd/praca_inz/wandb/run-20210415_115046-1rtpcijt/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train_loss_step</td><td>0.53111</td></tr><tr><td>train_acc_step</td><td>0.8125</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>_runtime</td><td>3661</td></tr><tr><td>_timestamp</td><td>1618483907</td></tr><tr><td>_step</td><td>999</td></tr><tr><td>train_loss_epoch</td><td>0.67116</td></tr><tr><td>train_acc_epoch</td><td>0.56606</td></tr><tr><td>training_epoch_end_accuracy</td><td>0.56606</td></tr><tr><td>training_epoch_end_loss</td><td>0.67116</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>validation_epoch_end_accuracy</td><td>0.59685</td></tr><tr><td>validation_epoch_end_loss</td><td>0.67759</td></tr><tr><td>test_epoch_end_accuracy</td><td>0.58984</td></tr><tr><td>test_epoch_end_loss</td><td>0.67902</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train_loss_step</td><td>▆▆██▇▇▅▅▅▁</td></tr><tr><td>train_acc_step</td><td>▅▄▂▁▁▃▅▅▅█</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▂▂▃▄▅▅▆▇██</td></tr><tr><td>_timestamp</td><td>▁▂▂▃▄▅▅▆▇██</td></tr><tr><td>_step</td><td>▁▁▂▂▂▃▃▄▄▄█</td></tr><tr><td>train_loss_epoch</td><td>▁</td></tr><tr><td>train_acc_epoch</td><td>▁</td></tr><tr><td>training_epoch_end_accuracy</td><td>▁</td></tr><tr><td>training_epoch_end_loss</td><td>▁</td></tr><tr><td>lr</td><td>▁</td></tr><tr><td>validation_epoch_end_accuracy</td><td>▁</td></tr><tr><td>validation_epoch_end_loss</td><td>▁</td></tr><tr><td>test_epoch_end_accuracy</td><td>▁</td></tr><tr><td>test_epoch_end_loss</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">misunderstood-dawn-13</strong>: <a href=\"https://wandb.ai/ulatwo/birdVox-NeuralNetwork_withAG/runs/1rtpcijt\" target=\"_blank\">https://wandb.ai/ulatwo/birdVox-NeuralNetwork_withAG/runs/1rtpcijt</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "wandb_logger = WandbLogger(project=\"birdVox-NeuralNetwork_withAG\")\n",
    "\n",
    "# przykładowe ścieżki:\n",
    "csv_path= './BirdVox/BirdVoxDCASE20k.csv'\n",
    "file_path='./BirdVox/data/wav'\n",
    "\n",
    "#batch_size ~ 32, 64 [32-128] to standard\n",
    "batch_size = 32\n",
    "\n",
    "#num_workers = 24 if cpu\n",
    "num_workers = 0\n",
    "\n",
    "\n",
    "# z ograniczeniem epok:\n",
    "trainer = pl.Trainer(\n",
    "    logger = wandb_logger,  #W&B integration\n",
    "    log_every_n_steps = 50, #set the logging frequency\n",
    "    max_epochs=1,           #number of epochs  \n",
    "    gpus =0,\n",
    "    progress_bar_refresh_rate=50\n",
    ")\n",
    "\n",
    "birdvox_dm = BirdVoxDataModule(csv_path, file_path, batch_size, num_workers)\n",
    "model = CNN_Audio_Model()\n",
    "\n",
    "trainer.fit(model, birdvox_dm)\n",
    "trainer.save_checkpoint(\"model_50e_AG.ckpt\")\n",
    "\n",
    "result = trainer.test(model)\n",
    "print(result)\n",
    "\n",
    "print(\"prediction_changed_by_AG_training: \", model.prediction_changed_by_AG_training )\n",
    "print(\"prediction_changed_by_AG_validation: \", model.prediction_changed_by_AG_validation )\n",
    "print(\"prediction_changed_by_AG_testing : \", model.prediction_changed_by_AG_testing )\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
